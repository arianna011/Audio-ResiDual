{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from latentis import PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir: Path = PROJECT_ROOT / \"results\" / \"exp3\"\n",
    "exp_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = list(exp_dir.glob(\"*\"))\n",
    "len([exp.name for exp in experiments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "df = defaultdict(list)\n",
    "for exp_path in experiments:\n",
    "    # if \"openclip\" not in exp_path.name:\n",
    "    #     continue\n",
    "    if not exp_path.is_file():\n",
    "        continue\n",
    "    exp_data = torch.load(exp_path, map_location=\"cpu\", weights_only=False)\n",
    "    for ablation in exp_data[\"ablations\"]:\n",
    "        for k, v in ablation.items():\n",
    "            if k == \"keep_units\" or k == \"ablated_shape\" or k == \"decomp\":\n",
    "                continue\n",
    "            if k == \"residual_indices\":\n",
    "                df[\"n_units\"].append(v.numel())\n",
    "            df[k].append(v if not isinstance(v, torch.Tensor) else v.numpy())\n",
    "        df[\"model\"].append(exp_data[\"model_name\"])\n",
    "        df[\"dataset\"].append(exp_data[\"dataset_name\"])\n",
    "df = pd.DataFrame(df)\n",
    "df.drop_duplicates(subset=[\"model\", \"dataset\", \"type\", \"ablation\"], inplace=True)\n",
    "df.drop(columns=[\"decomp\", \"residual_indices\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f\"greedy_{perc}%_corr_full_out_heads\": \"U\",\n",
    "# f\"greedy_{perc}%_corr_task_heads\": \"U|T\",\n",
    "# f\"greedy_{perc}%_supervised_heads\": \"S\",\n",
    "# f\"greedy_{perc}%_random_0_heads\": \"R\",\n",
    "df.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from residual.data.data_registry import dataset_names\n",
    "from residual.nn.model_registry import model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = 5\n",
    "for selection_type, selection_label in (\n",
    "    (f\"greedy_{perc}%_corr_full_out_heads\", \"U\"),\n",
    "    (f\"greedy_{perc}%_corr_task_heads\", \"U|T\"),\n",
    "    (f\"greedy_{perc}%_supervised_heads\", \"S\"),\n",
    "):\n",
    "    x = df[(df[\"type\"] == selection_type) & (df[\"ablation\"] == \"zero\")]\n",
    "    for encoder in x[\"model\"].unique():\n",
    "        if encoder != \"openclip_l\":\n",
    "            continue\n",
    "        encoder_x = x[x[\"model\"] == encoder][[\"dataset\", \"residual_indices\"]].to_dict(\n",
    "            orient=\"records\"\n",
    "        )\n",
    "        dataset2indices = dict(\n",
    "            list(\n",
    "                zip(\n",
    "                    [d[\"dataset\"] for d in encoder_x],\n",
    "                    [d[\"residual_indices\"] for d in encoder_x],\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        jaccard_matrix = torch.zeros((len(encoder_x), len(encoder_x)))\n",
    "        # compute rank-weighted jaccard similarity\n",
    "        for i, (_dataset1, indices1) in enumerate(dataset2indices.items()):\n",
    "            for j, (_dataset2, indices2) in enumerate(dataset2indices.items()):\n",
    "                jaccard_matrix[i, j] = len(set(indices1).intersection(indices2)) / len(\n",
    "                    set(indices1).union(indices2)\n",
    "                )\n",
    "\n",
    "        encoder_label = model_names[encoder]\n",
    "        dataset_labels = [dataset_names[d[\"dataset\"]] for d in encoder_x]\n",
    "\n",
    "        plt.imshow(jaccard_matrix.numpy(), cmap=\"Blues\")\n",
    "        plt.title(f\"{encoder_label} - {selection_label} - {perc}%\")\n",
    "        plt.xticks(range(len(encoder_x)), dataset_labels, rotation=45)\n",
    "        plt.yticks(range(len(encoder_x)), dataset_labels)\n",
    "        # add values in each cell\n",
    "        for i in range(len(encoder_x)):\n",
    "            for j in range(len(encoder_x)):\n",
    "                color = \"w\" if jaccard_matrix[i, j] > 0.5 else \"k\"\n",
    "                plt.text(\n",
    "                    j,\n",
    "                    i,\n",
    "                    f\"{jaccard_matrix[i, j]:.2f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=color,\n",
    "                )\n",
    "        plt.colorbar()\n",
    "        plt.savefig(\n",
    "            PROJECT_ROOT / \"results\" / f\"{encoder}_{selection_type}_jaccard.pdf\",\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0,\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"selection_method\"] = df[\"type\"].apply(\n",
    "    lambda x: \"_\".join(x.split(\"_\")[2:]) if x.startswith(\"greedy\") else \"manual\"\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"n_units\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"selection_method\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "runs = api.runs(\"resi_dual/residual\", filters={\"config.exp_type\": \"residual_coarse\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from latentis.space import Space\n",
    "from wandb.apis.public.runs import Run\n",
    "\n",
    "from residual.sparse_decomposition import SOMP\n",
    "\n",
    "coarse_data = []\n",
    "for run in runs:\n",
    "    run: Run\n",
    "\n",
    "    exp_type: str = run.config[\"exp_type\"]\n",
    "    dataset: str = run.config[\"dataset_name\"]\n",
    "    model: str = run.config.get(\"model_name\", None)\n",
    "    if model is None:\n",
    "        model = run.config[\"encoder_name\"]\n",
    "\n",
    "    run_data = {\n",
    "        \"model\": model,\n",
    "        \"dataset\": dataset,\n",
    "        \"type\": exp_type,\n",
    "        \"score\": run.summary.get(\"test/accuracy\", None),\n",
    "        \"selection_method\": \"optimized\",\n",
    "    }\n",
    "    encoding_path = (\n",
    "        PROJECT_ROOT / \"optimized\" / dataset / \"test\" / f\"{model}_{exp_type}_encodings\"\n",
    "    )\n",
    "    dictionary_path = PROJECT_ROOT / \"dictionaries\" / \"textspan\" / f\"{model}.pt\"\n",
    "\n",
    "    if encoding_path.exists() and dictionary_path.exists():\n",
    "        space = Space.load_from_disk(path=encoding_path).as_tensor()\n",
    "        decomposition = SOMP(k=10)\n",
    "\n",
    "        decomp_dictionary = torch.load(\n",
    "            dictionary_path,\n",
    "            weights_only=False,\n",
    "            # map_location=device,\n",
    "        )\n",
    "\n",
    "        decomp_out = decomposition(\n",
    "            X=space,\n",
    "            dictionary=F.normalize(decomp_dictionary[\"encodings\"]),\n",
    "            descriptors=decomp_dictionary[\"dictionary\"],\n",
    "            device=\"cpu\",\n",
    "        )\n",
    "        run_data[\"descriptions\"] = [str(x) for x in decomp_out[\"results\"]]\n",
    "\n",
    "    coarse_data.append(run_data)\n",
    "coarse_data = pd.DataFrame(coarse_data)\n",
    "\n",
    "coarse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.copy()\n",
    "# filtered_df = filtered_df[filtered_df[\"model\"].isin({\"openclip_l\", \"blip_l_flickr\"})]\n",
    "filtered_df[\"descriptions\"] = filtered_df[\"decomp\"].apply(\n",
    "    lambda x: x[\"results\"][:10] if x is not None else None\n",
    ")\n",
    "filtered_df.drop(columns=[\"decomp\"], inplace=True)\n",
    "filtered_df.drop(\"residual_indices\", axis=1, inplace=True)\n",
    "filtered_df = filtered_df[(filtered_df[\"ablation\"] != \"mean\")]\n",
    "perc = 5\n",
    "# filtered_df = filtered_df[\n",
    "#     (\n",
    "#         filtered_df[\"type\"].str.contains(f\"greedy_{perc}%_random\")\n",
    "#         | ~filtered_df[\"type\"].str.contains(\"random\")\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "types = {\n",
    "    f\"greedy_{perc}%_corr_full_out_heads\": \"U\",\n",
    "    f\"greedy_{perc}%_corr_task_heads\": \"U|T\",\n",
    "    f\"greedy_{perc}%_supervised_heads\": \"S\",\n",
    "    f\"greedy_{perc}%_random_0_heads\": \"R\",\n",
    "    # \"random_mean\": \"R\",\n",
    "    \"heads\": \"H\",\n",
    "    \"units\": \"B\",\n",
    "    \"residual_coarse\": \"O\",\n",
    "    # **{f\"greedy_10%_random_{i}_heads\": \"R\" for i in range(10)},\n",
    "}\n",
    "\n",
    "random_rows = filtered_df[filtered_df[\"type\"].str.contains(\"random\")]\n",
    "\n",
    "# Step 1: Filter rows where 'type' contains 'random'\n",
    "filtered_df = filtered_df[filtered_df[\"type\"].isin(types.keys())]\n",
    "\n",
    "# Step 2: Group by 'model' and 'dataset' and calculate mean and std for each group\n",
    "grouped_random = random_rows.groupby([\"model\", \"dataset\"])\n",
    "\n",
    "# Initialize an empty DataFrame to store results\n",
    "result_df = filtered_df[~filtered_df[\"type\"].str.contains(\"random\")].copy()\n",
    "\n",
    "# Initialize a list to collect new rows\n",
    "new_rows = []\n",
    "\n",
    "# Loop through each group and calculate the mean and std, then append to the result dataframe\n",
    "# for (model, dataset), group in grouped_random:\n",
    "#     # Calculate mean and std for the group\n",
    "#     group_mean = group.mean(numeric_only=True)\n",
    "#     group_std = group.std(numeric_only=True)\n",
    "\n",
    "#     # Prepare new rows for mean and std\n",
    "#     mean_row = pd.Series(group_mean, name=f\"random_mean_{model}_{dataset}\")\n",
    "#     std_row = pd.Series(group_std, name=f\"random_std_{model}_{dataset}\")\n",
    "\n",
    "#     # Add 'model' and 'dataset' information\n",
    "#     mean_row[\"model\"] = model\n",
    "#     mean_row[\"dataset\"] = dataset\n",
    "#     mean_row[\"ablation\"] = \"zero\"\n",
    "#     mean_row[\"type\"] = \"random_mean\"\n",
    "\n",
    "#     std_row[\"model\"] = model\n",
    "#     std_row[\"dataset\"] = dataset\n",
    "#     std_row[\"ablation\"] = \"zero\"\n",
    "#     std_row[\"type\"] = \"random_std\"\n",
    "\n",
    "#     # Append the mean and std rows to the list of new rows\n",
    "#     new_rows.append(mean_row)\n",
    "#     new_rows.append(std_row)\n",
    "\n",
    "# Step 3: Convert the list of new rows into a DataFrame and concatenate with the result dataframe\n",
    "# new_rows_df = pd.DataFrame(new_rows)\n",
    "# result_df = pd.concat([result_df, new_rows_df], ignore_index=True)\n",
    "# result_df = result_df[result_df[\"type\"] != \"random_std\"]\n",
    "\n",
    "result_df = filtered_df.copy()\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat(\n",
    "    [result_df, coarse_data[coarse_data[\"model\"].isin(result_df[\"model\"].unique())]],\n",
    "    ignore_index=True,\n",
    ")\n",
    "result_df.sort_values(by=[\"model\", \"dataset\", \"type\"], inplace=True)\n",
    "result_df[\"model\"] = result_df[\"model\"].apply(lambda x: model_names[x])\n",
    "result_df[\"dataset\"] = result_df[\"dataset\"].apply(lambda x: dataset_names[x])\n",
    "\n",
    "result_df[\"type\"] = result_df[\"type\"].apply(types.__getitem__)\n",
    "result_df[\"type\"] = pd.Categorical(result_df[\"type\"], categories=types.values())\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_df = result_df.explode(\"descriptions\").reset_index(drop=True)\n",
    "\n",
    "# Add a progressive ID for each sublist\n",
    "descriptions_df[\"description_id\"] = (\n",
    "    descriptions_df.groupby([\"model\", \"type\", \"dataset\"], observed=True).cumcount() + 1\n",
    ")\n",
    "descriptions_df = descriptions_df[descriptions_df[\"description_id\"] <= 3]\n",
    "# exploded_df = exploded_df[exploded_df[\"type\"] != \"R\"]\n",
    "descriptions_df = descriptions_df.rename({\"descriptions\": \"description\"}, axis=1)\n",
    "descriptions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_table = descriptions_df.pivot(\n",
    "    index=[\"dataset\", \"model\", \"type\"], columns=[\"description_id\"], values=\"description\"\n",
    ").fillna(0)\n",
    "descriptions_table = descriptions_table.to_latex(\n",
    "    multirow=True, column_format=\"c\", multicolumn_format=\"c\", float_format=\"%.2f\"\n",
    ")\n",
    "print(descriptions_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = result_df.pivot(\n",
    "    index=[\"dataset\"], columns=[\"model\", \"type\"], values=\"score\"\n",
    ").fillna(0)\n",
    "# reorder columns\n",
    "table = table[\n",
    "    sorted(\n",
    "        table.columns,\n",
    "        key=lambda x: (x[0], list(types.values()).index(x[1])),\n",
    "    )\n",
    "]\n",
    "table.loc[\"Average\"] = table.mean()\n",
    "\n",
    "table = table.to_latex(\n",
    "    multirow=True, column_format=\"c\", multicolumn_format=\"c\", float_format=\"%.2f\"\n",
    ")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
