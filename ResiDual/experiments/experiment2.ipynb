{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from latentis import PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir: Path = PROJECT_ROOT / \"results\" / \"exp2\"\n",
    "exp_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = list(exp_dir.glob(\"*\"))\n",
    "len([exp.name for exp in experiments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams.update(bundles.tmlr2023())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from residual.data.data_registry import dataset_names\n",
    "\n",
    "metric = \"spectral_distances\"\n",
    "datasets = list(dataset_names.keys())\n",
    "models = [\n",
    "    \"openclip_l\",\n",
    "    \"clip_l\",\n",
    "    \"blip_l_flickr\",\n",
    "    \"blip_l_coco\",\n",
    "    \"dinov2_l\",\n",
    "    \"vit_l\",\n",
    "]\n",
    "\n",
    "data = []\n",
    "for exp in experiments:\n",
    "    exp_data = torch.load(exp, map_location=\"cpu\", weights_only=True)\n",
    "    dataset1 = exp_data[\"dataset1\"]\n",
    "    dataset2 = exp_data[\"dataset2\"]\n",
    "    encoder = exp_data[\"encoder_name\"]\n",
    "\n",
    "    if dataset1 != \"imagenet\":\n",
    "        continue\n",
    "\n",
    "    if dataset2 in datasets and encoder in models:\n",
    "        spectral_distances = exp_data[\"spectral_distances\"]\n",
    "        distances_shape = spectral_distances.shape\n",
    "\n",
    "        data.append(\n",
    "            {\n",
    "                \"encoder\": encoder,\n",
    "                \"dataset1\": dataset1,\n",
    "                \"dataset2\": dataset2,\n",
    "                \"spectral_distances\": spectral_distances,\n",
    "                \"distances_shape\": distances_shape,\n",
    "            }\n",
    "        )\n",
    "        # data.append(\n",
    "        #     {\n",
    "        #         \"encoder\": encoder,\n",
    "        #         \"dataset1\": dataset2,\n",
    "        #         \"dataset2\": dataset1,\n",
    "        #         \"spectral_distances\": spectral_distances,\n",
    "        #         \"distances_shape\": distances_shape,\n",
    "        #     }\n",
    "        # )\n",
    "data = pd.DataFrame(data)\n",
    "data = data.sort_values(by=[\"encoder\", \"dataset1\", \"dataset2\"])\n",
    "# visualize data without the \"spectral_distances\" column since it's too large\n",
    "data.drop(columns=[\"spectral_distances\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = torch.stack(data[\"spectral_distances\"].tolist(), dim=0).reshape(\n",
    "    len(data.encoder.unique()),\n",
    "    len(data.dataset2.unique()),\n",
    "    24,\n",
    "    16,\n",
    "    24,\n",
    "    16,\n",
    ")\n",
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure\n",
    "\n",
    "from residual.plot import blocked_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_index, model in enumerate(data.encoder.unique()):\n",
    "    model_similarities = similarities[model_index]\n",
    "    model_similarities = model_similarities.view(\n",
    "        model_similarities.shape[0],\n",
    "        model_similarities.shape[1] * model_similarities.shape[2],\n",
    "        -1,\n",
    "    )\n",
    "\n",
    "    # we are interested only in the similarity between corresponding units\n",
    "    model_similarities = model_similarities[\n",
    "        :,\n",
    "        torch.arange(model_similarities.shape[1]),\n",
    "        torch.arange(model_similarities.shape[1]),\n",
    "    ]\n",
    "\n",
    "    heatmap: Figure = blocked_heatmap(\n",
    "        data=model_similarities,\n",
    "        block_size=16,\n",
    "        y_labels=data.dataset2.unique().tolist(),\n",
    "    )\n",
    "    heatmap.suptitle(f\"{model}\")\n",
    "    heatmap.show()\n",
    "\n",
    "    heatmap.savefig(f\"{model}_allheads.pdf\", dpi=200, bbox_inches=\"tight\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
